{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 112, 112])\n",
      "torch.Size([1, 64, 112, 112])\n",
      "torch.Size([1, 72, 56, 56])\n",
      "torch.Size([1, 72, 56, 56])\n",
      "torch.Size([1, 120, 28, 28])\n",
      "torch.Size([1, 120, 28, 28])\n",
      "torch.Size([1, 240, 28, 28])\n",
      "torch.Size([1, 200, 14, 14])\n",
      "torch.Size([1, 184, 14, 14])\n",
      "torch.Size([1, 184, 14, 14])\n",
      "torch.Size([1, 480, 14, 14])\n",
      "torch.Size([1, 672, 14, 14])\n",
      "torch.Size([1, 672, 14, 14])\n",
      "torch.Size([1, 960, 7, 7])\n",
      "torch.Size([1, 960, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import mobilenet_v3_large\n",
    "import torch\n",
    "\n",
    "x = torch.randn([1, 3, 224, 224])\n",
    "model = mobilenet_v3_large()\n",
    "\n",
    "def hook(module, args, output):\n",
    "    print(args[0].shape)\n",
    "\n",
    "for mod in model.modules():\n",
    "    if isinstance(mod, torch.nn.Conv2d) and mod.groups == mod.in_channels:\n",
    "        mod.register_forward_hook(hook)\n",
    "\n",
    "model(x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(32, 32, kernel_size=3, padding=1, groups=32, bias=False),\n",
    "    nn.Conv2d(32, 64, kernel_size=1, bias=False)\n",
    ").cuda()\n",
    "\n",
    "x = torch.randn([1, 32, 64, 64], requires_grad=False).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 3, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[1].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vinnamkim/flash-fft-conv/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    if param.grad is not None:\n",
    "        param.grad.data.fill_(0.0)\n",
    "\n",
    "y = model(x)\n",
    "y.mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from monarch_cuda import conv2d_forward\n",
    "\n",
    "n, num_channels, h, w = 32, 16, 112, 112\n",
    "#n, num_channels, h, w = 16, 960, 7, 7\n",
    "n, num_channels, h, w = 32, 240, 28, 28\n",
    "# n = 4\n",
    "# num_channels = 512\n",
    "# h = w = 64\n",
    "\n",
    "depthwise_conv2d = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1, groups=num_channels, bias=False).cuda()\n",
    "x = torch.randn([n, num_channels, h, w], requires_grad=False).cuda()\n",
    "\n",
    "@torch.no_grad()\n",
    "def run():\n",
    "    y = depthwise_conv2d(x)\n",
    "    torch.cuda.synchronize()\n",
    "    return y\n",
    "\n",
    "y = run()\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_my():\n",
    "    y = conv2d_forward(x, depthwise_conv2d.weight.contiguous(), 1)\n",
    "    torch.cuda.synchronize()\n",
    "    return y\n",
    "\n",
    "z = run_my()\n",
    "\n",
    "print(torch.allclose(y, z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.17 ms ± 61.7 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1000 run_my()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.72 ms ± 8.97 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1000 run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'grad_input': (tensor([[[[ 1.1685,  1.3499,  0.4227,  0.2620,  0.8351],\n",
      "          [ 0.9209,  2.0667,  0.4926,  1.3879,  1.5142],\n",
      "          [-0.4524,  1.5968,  2.3990, -0.5796,  1.1033],\n",
      "          [-0.4179,  0.5972,  1.9350,  0.2136,  0.7750],\n",
      "          [ 0.0033, -0.5234,  0.4973,  1.2309,  0.6199]],\n",
      "\n",
      "         [[-0.6326, -0.5939, -0.1359, -0.0636,  0.0743],\n",
      "          [-0.1904, -0.5326, -0.9812, -1.1572, -0.3421],\n",
      "          [-0.3705,  0.3074, -0.0607, -0.3725, -0.1068],\n",
      "          [-0.5645, -0.7567, -0.9802, -0.3866,  0.2590],\n",
      "          [ 0.0823, -0.0074, -0.2114,  0.2522,  0.8198]]]], device='cuda:0'),), 'grad_output': (tensor([[[[1.8389, 1.4754, 0.7583, 0.8973, 1.1330],\n",
      "          [1.4409, 2.0260, 1.6827, 0.4276, 1.5788],\n",
      "          [0.4251, 3.3738, 0.6538, 1.0533, 3.3021],\n",
      "          [0.7232, 0.5160, 2.7773, 0.8202, 0.4945],\n",
      "          [0.5850, 0.3772, 1.7576, 1.0424, 0.4658]],\n",
      "\n",
      "         [[0.4863, 0.7108, 1.1429, 1.1681, 1.4493],\n",
      "          [2.3139, 1.0769, 0.4998, 0.3559, 0.9162],\n",
      "          [1.0466, 0.9155, 2.0475, 2.3662, 1.4991],\n",
      "          [0.3858, 0.2704, 1.0408, 1.9785, 1.0602],\n",
      "          [1.0664, 0.6745, 1.3839, 1.6645, 1.3730]]]], device='cuda:0'),)}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from monarch_cuda import conv2d_backward\n",
    "\n",
    "# n, num_channels, h, w = 32, 16, 112, 112\n",
    "#n, num_channels, h, w = 16, 960, 7, 7\n",
    "# n, num_channels, h, w = 32, 240, 28, 28\n",
    "n, num_channels, h, w = 1, 2, 5, 5\n",
    "# n = 4\n",
    "# num_channels = 512\n",
    "# h = w = 64\n",
    "\n",
    "depthwise_conv2d = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1, groups=num_channels, bias=False).cuda()\n",
    "x = torch.randn([n, num_channels, h, w], requires_grad=True).cuda()\n",
    "\n",
    "states = {}\n",
    "def log_bwd(module, grad_input, grad_output):\n",
    "    states[\"grad_input\"] = grad_input\n",
    "    states[\"grad_output\"] = grad_output\n",
    "\n",
    "depthwise_conv2d.register_full_backward_hook(log_bwd)\n",
    "\n",
    "y = depthwise_conv2d(x)\n",
    "y.exp().sum().backward()\n",
    "\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'grad_input': (tensor([[[[ 1.1685,  1.3499,  0.4227,  0.2620,  0.8351],\n",
      "          [ 0.9209,  2.0667,  0.4926,  1.3879,  1.5142],\n",
      "          [-0.4524,  1.5968,  2.3990, -0.5796,  1.1033],\n",
      "          [-0.4179,  0.5972,  1.9350,  0.2136,  0.7750],\n",
      "          [ 0.0033, -0.5234,  0.4973,  1.2309,  0.6199]],\n",
      "\n",
      "         [[-0.6326, -0.5939, -0.1359, -0.0636,  0.0743],\n",
      "          [-0.1904, -0.5326, -0.9812, -1.1572, -0.3421],\n",
      "          [-0.3705,  0.3074, -0.0607, -0.3725, -0.1068],\n",
      "          [-0.5645, -0.7567, -0.9802, -0.3866,  0.2590],\n",
      "          [ 0.0823, -0.0074, -0.2114,  0.2522,  0.8198]]]], device='cuda:0'),), 'grad_output': (tensor([[[[1.8389, 1.4754, 0.7583, 0.8973, 1.1330],\n",
      "          [1.4409, 2.0260, 1.6827, 0.4276, 1.5788],\n",
      "          [0.4251, 3.3738, 0.6538, 1.0533, 3.3021],\n",
      "          [0.7232, 0.5160, 2.7773, 0.8202, 0.4945],\n",
      "          [0.5850, 0.3772, 1.7576, 1.0424, 0.4658]],\n",
      "\n",
      "         [[0.4863, 0.7108, 1.1429, 1.1681, 1.4493],\n",
      "          [2.3139, 1.0769, 0.4998, 0.3559, 0.9162],\n",
      "          [1.0466, 0.9155, 2.0475, 2.3662, 1.4991],\n",
      "          [0.3858, 0.2704, 1.0408, 1.9785, 1.0602],\n",
      "          [1.0664, 0.6745, 1.3839, 1.6645, 1.3730]]]], device='cuda:0'),)}\n",
      "tensor([[[[ 1.1685,  1.3499,  0.4227,  0.2620,  0.8351],\n",
      "          [ 0.9209,  2.0667,  0.4926,  1.3879,  1.5142],\n",
      "          [-0.4524,  1.5968,  2.3990, -0.5796,  1.1033],\n",
      "          [-0.4179,  0.5972,  1.9350,  0.2136,  0.7750],\n",
      "          [ 0.0033, -0.5234,  0.4973,  1.2309,  0.6199]],\n",
      "\n",
      "         [[-0.6326, -0.5939, -0.1359, -0.0636,  0.0743],\n",
      "          [-0.1904, -0.5326, -0.9812, -1.1572, -0.3421],\n",
      "          [-0.3705,  0.3074, -0.0607, -0.3725, -0.1068],\n",
      "          [-0.5645, -0.7567, -0.9802, -0.3866,  0.2590],\n",
      "          [ 0.0823, -0.0074, -0.2114,  0.2522,  0.8198]]]], device='cuda:0')\n",
      "{'grad_input': (tensor([[[[ 1.1685,  1.3499,  0.4227,  0.2620,  0.8351],\n",
      "          [ 0.9209,  2.0667,  0.4926,  1.3879,  1.5142],\n",
      "          [-0.4524,  1.5968,  2.3990, -0.5796,  1.1033],\n",
      "          [-0.4179,  0.5972,  1.9350,  0.2136,  0.7750],\n",
      "          [ 0.0033, -0.5234,  0.4973,  1.2309,  0.6199]],\n",
      "\n",
      "         [[-0.6326, -0.5939, -0.1359, -0.0636,  0.0743],\n",
      "          [-0.1904, -0.5326, -0.9812, -1.1572, -0.3421],\n",
      "          [-0.3705,  0.3074, -0.0607, -0.3725, -0.1068],\n",
      "          [-0.5645, -0.7567, -0.9802, -0.3866,  0.2590],\n",
      "          [ 0.0823, -0.0074, -0.2114,  0.2522,  0.8198]]]], device='cuda:0'),), 'grad_output': (tensor([[[[1.8389, 1.4754, 0.7583, 0.8973, 1.1330],\n",
      "          [1.4409, 2.0260, 1.6827, 0.4276, 1.5788],\n",
      "          [0.4251, 3.3738, 0.6538, 1.0533, 3.3021],\n",
      "          [0.7232, 0.5160, 2.7773, 0.8202, 0.4945],\n",
      "          [0.5850, 0.3772, 1.7576, 1.0424, 0.4658]],\n",
      "\n",
      "         [[0.4863, 0.7108, 1.1429, 1.1681, 1.4493],\n",
      "          [2.3139, 1.0769, 0.4998, 0.3559, 0.9162],\n",
      "          [1.0466, 0.9155, 2.0475, 2.3662, 1.4991],\n",
      "          [0.3858, 0.2704, 1.0408, 1.9785, 1.0602],\n",
      "          [1.0664, 0.6745, 1.3839, 1.6645, 1.3730]]]], device='cuda:0'),)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(states)\n",
    "with torch.no_grad():\n",
    "    din, dweights = conv2d_backward(states[\"grad_output\"][0].clone(), x.clone(), depthwise_conv2d.weight.clone(), 1)\n",
    "print(din)\n",
    "print(states)\n",
    "# print(din.shape)\n",
    "# print(dweights)\n",
    "# print(dweights.shape)\n",
    "torch.allclose(states[\"grad_input\"][0], din)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.4013, 1.7406, 1.3197],\n",
       "          [0.9091, 1.2141, 1.3131],\n",
       "          [0.9499, 0.8774, 1.3109]],\n",
       "\n",
       "         [[1.4815, 1.4291, 1.0360],\n",
       "          [0.6996, 0.6797, 0.4451],\n",
       "          [0.9602, 2.1373, 1.6364]]]], device='cuda:0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states[\"grad_output\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0.2053, 0.3073, 0.4549],\n",
       "           [0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000],\n",
       "           [0.0000, 0.0000, 0.0000]]]], device='cuda:0'),)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states[\"grad_input\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
